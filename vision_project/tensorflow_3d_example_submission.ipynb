{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nER__IitSUFT",
        "outputId": "2b87af7a-11e0-45ce-fcd9-056343dd3911"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BzDlZFqf2xig"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import albumentations as A\n",
        "import glob\n",
        "import re\n",
        "import pydicom\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = [512, 512]\n",
        "IN_CHANS = 10\n",
        "N_LABELS = 25\n",
        "N_CLASSES = 3 * N_LABELS\n",
        "rd = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\""
      ],
      "metadata": {
        "id": "6Cgl302XSk-1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSNA24TestDataset(Sequence):\n",
        "    def __init__(self, df, study_ids, batch_size=1, phase='test', transform=None):\n",
        "        self.df = df\n",
        "        self.study_ids = study_ids\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.study_ids) // self.batch_size\n",
        "\n",
        "    def get_img_paths(self, study_id, series_desc):\n",
        "        pdf = self.df[self.df['study_id'] == study_id]\n",
        "        pdf_ = pdf[pdf['series_description'] == series_desc]\n",
        "        allimgs = []\n",
        "        for _, row in pdf_.iterrows():\n",
        "            pimgs = glob.glob(f'{rd}/test_images/{study_id}/{row[\"series_id\"]}/*.dcm')\n",
        "            pimgs = sorted(pimgs, key=natural_keys)\n",
        "            allimgs.extend(pimgs)\n",
        "        return allimgs\n",
        "\n",
        "    def read_dcm_ret_arr(self, src_path):\n",
        "        dicom_data = pydicom.dcmread(src_path)\n",
        "        image = dicom_data.pixel_array\n",
        "        image = cv2.resize(image, (IMG_SIZE[1], IMG_SIZE[0]))\n",
        "        image = image / 255.0\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        study_id = self.study_ids[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "        series_desc = list(self.df[self.df['study_id'] == study_id[0]][\"series_description\"].unique())\n",
        "        imgs = []\n",
        "        for desc in series_desc:\n",
        "            img_paths = self.get_img_paths(study_id[0], desc)\n",
        "            for img_path in img_paths:\n",
        "                img = self.read_dcm_ret_arr(img_path)\n",
        "                imgs.append(img)\n",
        "        imgs = np.array(imgs)\n",
        "        if self.transform:\n",
        "            imgs = self.transform(image=imgs)[\"image\"]\n",
        "        imgs = np.expand_dims(imgs, axis=-1)\n",
        "        return imgs, study_id\n"
      ],
      "metadata": {
        "id": "lqXTcppE27Cu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition"
      ],
      "metadata": {
        "id": "mqoGJdaG3CpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv3D(32, kernel_size=(3, 3, 3), strides=1, padding='same', activation='relu', input_shape=(IN_CHANS, IMG_SIZE[0], IMG_SIZE[1], 1)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "    model.add(layers.Conv3D(64, kernel_size=(3, 3, 3), strides=1, padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "    model.add(layers.Conv3D(128, kernel_size=(3, 3, 3), strides=1, padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "    model.add(layers.Conv3D(256, kernel_size=(3, 3, 3), strides=1, padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2)))\n",
        "\n",
        "    model.add(layers.Conv3D(512, kernel_size=(3, 3, 3), strides=1, padding='same', activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(N_CLASSES, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary to check the layers and output shapes\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnOTWASm2_gt",
        "outputId": "bb16a204-e83a-4b3a-a1da-d8d9b0fedea4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_8 (Conv3D)           (None, 10, 512, 512, 32   896       \n",
            "                             )                                   \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 10, 512, 512, 32   128       \n",
            " chNormalization)            )                                   \n",
            "                                                                 \n",
            " max_pooling3d_7 (MaxPoolin  (None, 10, 256, 256, 32   0         \n",
            " g3D)                        )                                   \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 10, 256, 256, 64   55360     \n",
            "                             )                                   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 10, 256, 256, 64   256       \n",
            " chNormalization)            )                                   \n",
            "                                                                 \n",
            " max_pooling3d_8 (MaxPoolin  (None, 10, 128, 128, 64   0         \n",
            " g3D)                        )                                   \n",
            "                                                                 \n",
            " conv3d_10 (Conv3D)          (None, 10, 128, 128, 12   221312    \n",
            "                             8)                                  \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 10, 128, 128, 12   512       \n",
            " chNormalization)            8)                                  \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPoolin  (None, 10, 64, 64, 128)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 10, 64, 64, 256)   884992    \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 10, 64, 64, 256)   1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPooli  (None, 10, 32, 32, 256)   0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " conv3d_12 (Conv3D)          (None, 10, 32, 32, 512)   3539456   \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 10, 32, 32, 512)   2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPooli  (None, 5, 16, 16, 512)    0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 655360)            0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               167772416 \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 75)                9675      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 172652555 (658.62 MB)\n",
            "Trainable params: 172650571 (658.61 MB)\n",
            "Non-trainable params: 1984 (7.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading weights and creating predictions"
      ],
      "metadata": {
        "id": "_09vHMjK3R3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/path/to/your/tensorflow_model.h5') # we need to add in our model\n",
        "\n",
        "def create_predictions(model, test_dataset):\n",
        "    outputs = []\n",
        "    for inputs, study in test_dataset:\n",
        "        inputs = np.expand_dims(inputs, axis=0)  # Add batch dimension\n",
        "        output = model.predict(inputs)\n",
        "        outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "test_dataset = RSNA24TestDataset(df, study_ids)\n",
        "predictions = create_predictions(model, test_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "D5ZM0lpa3FoZ",
        "outputId": "53bfd10f-7f2b-4814-adaa-dbc10759aeb1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "No file or directory found at /path/to/your/tensorflow_model.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3778670bae06>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/your/tensorflow_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /path/to/your/tensorflow_model.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the predictions"
      ],
      "metadata": {
        "id": "PHkrc7Vd3ZGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_pred = []\n",
        "for prediction in predictions:\n",
        "    o_pred = tf.reshape(prediction, (25, 3))\n",
        "    o_pred = tf.nn.softmax(o_pred, axis=1)\n",
        "    output_pred.append(o_pred)\n",
        "\n",
        "predictions_f = []\n",
        "for prediction in output_pred:\n",
        "    prediction = prediction.numpy()\n",
        "    predictions_f.append(prediction[:, [1, 0, 2]])\n",
        "\n",
        "submission_rows = []\n",
        "for study in study_ids:\n",
        "    for condition in submission_columns:\n",
        "        new_c = condition.lower().replace(\"/\", \"_\")\n",
        "        submission_rows.append(str(study) + \"_\" + new_c)\n",
        "\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df[LABELS[0]] = submission_rows\n",
        "\n",
        "preds_df = pd.DataFrame(columns=LABELS[1:4])\n",
        "for pred in predictions_f:\n",
        "    pred = pd.DataFrame(pred, columns=LABELS[1:4])\n",
        "    preds_df = pd.concat([preds_df, pred], axis=0, ignore_index=True)\n",
        "\n",
        "final_submission = pd.concat([submission_df, preds_df], axis=1)\n",
        "final_submission = final_submission.sort_values(by=\"row_id\")\n",
        "final_submission.to_csv(\"submission.csv\", index=False)\n",
        "pd.read_csv('submission.csv')\n"
      ],
      "metadata": {
        "id": "CiDMYqvr3W0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYuH5k5T3c6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}